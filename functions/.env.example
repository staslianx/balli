#
# Balli Functions Environment Configuration
# Copy this to .env and configure as needed
#

# Provider Selection (true = Vertex AI with caching, false = Google AI)
USE_VERTEX_AI=false

# Google AI Configuration (when USE_VERTEX_AI=false)
# Get your API key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# Google Cloud Project Configuration (when USE_VERTEX_AI=true)
GOOGLE_CLOUD_PROJECT_ID=balli-project

# Embedding Configuration (1536 uses Matryoshka MRL to reduce quota usage)
# Options: 768, 1536, or 3072 (gemini-embedding-001 supports these via MRL)
EMBEDDING_DIMENSIONS=1536

# Research Search API Keys
# Exa: Get your API key from https://exa.ai/
EXA_API_KEY=your_exa_api_key_here

# PubMed E-utilities: Get your API key from https://www.ncbi.nlm.nih.gov/account/
PUBMED_API_KEY=your_pubmed_api_key_here

# Note: ClinicalTrials.gov and arXiv APIs do not require API keys

# Note: Thinking budget is configured in .prompt files (thinking_budget: 0)

#
# Usage Instructions:
#
# 1. Development with Google AI (current setup):
#    USE_VERTEX_AI=false
#    GEMINI_API_KEY=your_key
#
# 2. Production with Vertex AI + Context Caching:
#    USE_VERTEX_AI=true
#    GOOGLE_CLOUD_PROJECT_ID=balli-project
#
#    Note: Vertex AI uses Application Default Credentials in Cloud Functions
#    No API key needed, just proper IAM permissions
#
# 3. Local Development with Vertex AI:
#    USE_VERTEX_AI=true
#    GOOGLE_CLOUD_PROJECT_ID=balli-project
#
#    Run: gcloud auth application-default login
#    Then: npm run dev
#

#
# Expected Cost Savings with Vertex AI + Caching:
# - System prompt caching: ~85% reduction (3000+ tokens cached)
# - Overall API costs: ~70-80% reduction for frequent users
# - Improved latency: ~10-50ms faster response times
#